# Project Architecture: Multi-Agent Tool Calling with MCP

This document details the architectural design of the multi-agent system, focusing on how the Multi-Server Communication Protocol (MCP) facilitates interaction between a central LangChain agent and various specialized tool servers.

## 1. High-Level Overview

The system operates on a client-server model, where a central `client.py` application acts as an intelligent agent. This agent does not directly implement tools but rather discovers and utilizes tools exposed by independent "tool servers." The communication between the client and these servers is managed by the Multi-Server Communication Protocol (MCP).
```bash
+-------------------+       +-------------------+
|                   |       |                   |
|   Client (Agent)  |<----->|   MCP Servers     |
|   (client.py)     |       |   (mathserver.py, |
|                   |       |    weather.py)    |
+-------------------+       +-------------------+
^                           ^
|                           |
| LangChain / Groq          | MCP (stdio, HTTP)
v                           v
+-------------------+       +-------------------+
|                   |       |                   |
|   LLM (Groq)      |       |   External APIs   |
|                   |       |   (OpenWeatherMap)|
+-------------------+       +-------------------+
```

## 2. Key Components

### 2.1. `client.py` (The Agent Orchestrator)

The `client.py` script is the core of the intelligent agent. Its responsibilities include:

* [cite_start]**Environment Setup**: Loads API keys for Groq and OpenWeatherMap from `.env` files[cite: 1].
* [cite_start]**MCP Client Initialization**: Initializes the `MultiServerMCPClient`[cite: 1]. [cite_start]This client is configured to connect to multiple MCP servers, each potentially using a different transport mechanism[cite: 1].
    * [cite_start]**Math Server Configuration**: Configured to connect to `mathserver.py` using the `stdio` transport[cite: 1]. This means the client will start `mathserver.py` as a subprocess and communicate with it via its standard input and output streams.
    * [cite_start]**Weather Server Configuration**: Configured to connect to the `weather.py` server via `streamable_http` transport at `http://localhost:3000`[cite: 1]. This implies `weather.py` is a web server exposing its tools over HTTP.
* [cite_start]**Tool Discovery**: Uses `client.get_tools()` to dynamically fetch a list of available tools from all connected MCP servers[cite: 1]. This allows the agent to be flexible and adapt to new tools without code changes.
* [cite_start]**LLM Integration**: Initializes `ChatGroq` with the "llama3-70b-8192" model[cite: 1]. This Large Language Model serves as the "brain" of the agent.
* [cite_start]**Agent Creation**: Creates a LangChain `react_agent` by providing the Groq model and the dynamically discovered tools[cite: 1]. The ReAct (Reasoning and Acting) framework enables the agent to reason about a task, select appropriate tools, execute them, and refine its thoughts based on the tool's output.
* [cite_start]**Query Execution**: Sends user queries to the agent, which then decides which tool(s) to use, executes them via the `MultiServerMCPClient`, and generates a final response[cite: 1].

### 2.2. `mathserver.py` (The Math Tool Server)

This is a simple MCP tool server dedicated to mathematical operations.

* **FastMCP Initialization**: Uses `FastMCP("Math")` to create an MCP server named "Math".
* **Tool Definition**: Exposes two tools:
    * `add(a: int, b: int) -> int`: Adds two numbers.
    * `multiple(a: int, b: int) -> int`: Multiplies two numbers.
* **Stdio Transport**: Runs using `mcp.run(transport="stdio")`. This means it communicates by reading from standard input and writing to standard output, making it suitable for direct process invocation by the `MultiServerMCPClient`.

### 2.3. `weather.py` (The Weather Tool Server)

This MCP tool server provides real-time weather information.

* **FastMCP Initialization**: Creates an MCP server named "Weather".
* **Tool Definition**: Exposes one tool:
    * `get_weather(location: str) -> str`: Fetches current weather data for a specified location.
* **External API Integration**: Internally, `get_weather` makes asynchronous HTTP requests to the OpenWeatherMap API using `aiohttp`. It handles API key validation and error cases (e.g., city not found).
* **Streamable HTTP Transport**: This server is designed to run as a web service.
    * It sets `os.environ["FASTMCP_PORT"] = "3000"` to specify its listening port.
    * It retrieves the FastAPI application generated by FastMCP using `mcp.streamable_http_app()`.
    * It explicitly runs the application using `uvicorn.run(app, host="127.0.0.1", port=3000)`. This makes it accessible over HTTP, allowing the `MultiServerMCPClient` to connect to it as a `streamable_http` transport.

## 3. Communication Flow

1.  **Client Startup**: `client.py` starts and initializes `MultiServerMCPClient`.
2.  **Server Launch (Implicit/Explicit)**:
    * For `mathserver.py` (stdio transport), the `MultiServerMCPClient` likely launches it as a subprocess.
    * For `weather.py` (streamable\_http transport), `weather.py` must be running independently (as a Uvicorn server) before `client.py` attempts to connect. The `client.py` code *does* launch the math server, but for the weather server, it relies on it already being available at the specified URL. In a full production setup, both might be managed by a process manager.
3.  **Tool Discovery**: `client.py` sends requests to both the Math and Weather MCP servers to discover their available tools (e.g., `add`, `multiple`, `get_weather`).
4.  **Agent Initialization**: The LangChain agent is initialized with the discovered tools.
5.  **User Query**: A user submits a query to `client.py` (e.g., "what is the weather in Hyderabad, India?").
6.  **Agent Reasoning**: The Groq LLM, as part of the React agent, analyzes the query and determines which tool is best suited to answer it (e.g., `get_weather`).
7.  **Tool Invocation**:
    * The agent constructs the appropriate tool call (e.g., `get_weather(location="Hyderabad, India")`).
    * The `MultiServerMCPClient` routes this call to the correct MCP server (`Weather` server).
    * For the `Weather` server, this involves an HTTP request to `http://localhost:3000` carrying the tool invocation payload.
    * For the `Math` server, this involves sending the tool invocation payload over the stdio pipe.
8.  **Tool Execution**: The respective MCP server receives the invocation, executes the actual tool function (`get_weather` or `add`/`multiple`), which might involve calling external APIs (OpenWeatherMap).
9.  **Result Return**: The tool's result is sent back to the `MultiServerMCPClient` via the corresponding transport (HTTP response or stdio output).
10. **Agent Response**: The `MultiServerMCPClient` relays the result to the LangChain agent. The agent then incorporates this result into its reasoning and generates a final, coherent response to the user.

## 4. Scalability and Extensibility

* **Modular Tool Servers**: Each tool server is an independent unit, allowing for easy addition of new functionalities without modifying the core client logic.
* **Diverse Transports**: The support for multiple transports (stdio, streamable\_http) enables integration with various types of services, from simple local scripts to full-fledged web APIs.
* **LLM Flexibility**: The LangChain framework allows for easy swapping of the underlying Large Language Model (e.g., to another Groq model or a different provider) if desired.
* **Parallel Tool Execution**: MCP's design, particularly with HTTP transports, allows for potential parallel execution of tool calls to different servers, improving overall response times for complex queries requiring multiple tools.

## 5. Considerations and Future Improvements

* **Error Handling and Retries**: While basic error handling is present, a more robust strategy for network errors, API rate limits, and server failures could be implemented.
* **Logging and Monitoring**: Comprehensive logging across all components (client and servers) and integration with monitoring tools would be crucial for debugging and operational visibility.
* **Security**: For production deployments, securing HTTP endpoints (e.g., with authentication/authorization), protecting API keys, and validating input thoroughly would be paramount.
* **Containerization**: Containerizing each server (e.g., using Docker) would simplify deployment and ensure consistent environments.
* **Configuration Management**: Externalizing more configuration (e.g., server URLs, ports) beyond just API keys would enhance flexibility.
